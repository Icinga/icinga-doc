<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!ENTITY % all.entities SYSTEM "all-entities.ent">
%all.entities;
]>
<section version="5.0" xml:id="distributed" xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:svg="http://www.w3.org/2000/svg"
         xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:html="http://www.w3.org/1999/xhtml"
         xmlns:db="http://docbook.org/ns/docbook">
  <title><anchor xml:id="distributed_monitoring" />Distributed Monitoring</title>

  <para><emphasis role="bold">Introduction</emphasis></para>

  <para>&name-icinga; can be configured to support distributed monitoring of network services and resources. We'll try to briefly
  explan how this can be accomplished...</para>

  <para><emphasis role="bold">Goals</emphasis></para>

  <para>The goal in the distributed monitoring environment that we will describe is to offload the overhead (CPU usage, etc.) of
  performing service checks from a "central" server onto one or more "distributed" servers. Most small to medium sized shops will
  not have a real need for setting up such an environment. However, when you want to start monitoring hundreds or even thousands
  of <emphasis>hosts</emphasis> (and several times that many services) using &name-icinga;, this becomes quite important.</para>

  <para><emphasis role="bold">Reference Diagram</emphasis></para>

  <para>The diagram below should help give you a general idea of how distributed monitoring works with &name-icinga;. We'll be
  referring to the items shown in the diagram as we explain things...</para>

  <para><inlinemediaobject>
      <imageobject>
        <imagedata fileref="../images/distributed.png" format="PNG"></imagedata>
      </imageobject>
    </inlinemediaobject></para>

  <para><emphasis role="bold">Central Server vs. Distributed Servers</emphasis></para>

  <para>When setting up a distributed monitoring environment with &name-icinga;, there are differences in the way the central and
  distributed servers are configured. We'll show you how to configure both types of servers and explain what effects the changes
  being made have on the overall monitoring. For starters, lets describe the purpose of the different types of servers...</para>

  <para>The function of a <emphasis>distributed server</emphasis> is to actively perform checks all the services you define for a
  "cluster" of hosts. We use the term "cluster" loosely - it basically just mean an arbitrary group of hosts on your network.
  Depending on your network layout, you may have several cluters at one physical location, or each cluster may be separated by a
  WAN, its own firewall, etc. The important thing to remember to that for each cluster of hosts (however you define that), there
  is one distributed server that runs &name-icinga; and monitors the services on the hosts in the cluster. A distributed server is
  usually a bare-bones installation of &name-icinga;. It doesn't have to have the web interface installed, send out notifications,
  run event handler scripts, or do anything other than execute service checks if you don't want it to. More detailed information
  on configuring a distributed server comes later...</para>

  <para>The purpose of the <emphasis>central server</emphasis> is to simply listen for service check results from one or more
  distributed servers. Even though services are occassionally actively checked from the central server, the active checks are only
  performed in dire circumstances, so lets just say that the central server only accepts passive check for now. Since the central
  server is obtaining <link linkend="passivechecks">passive service check</link> results from one or more distributed servers, it
  serves as the focal point for all monitoring logic (i.e. it sends out notifications, runs event handler scripts, determines host
  states, has the web interface installed, etc).</para>

  <para><emphasis role="bold">Obtaining Service Check Information From Distributed Monitors</emphasis></para>

  <para>Okay, before we go jumping into configuration detail we need to know how to send the service check results from the
  distributed servers to the central server. We've already discussed how to submit passive check results to &name-icinga; from
  same host that &name-icinga; is running on (as described in the documentation on <link linkend="passivechecks">passive
  checks</link>), but we haven't given any info on how to submit passive check results from other hosts.</para>

  <para>In order to facilitate the submission of passive check results to a remote host, the <link linkend="addons-nsca">nsca
  addon</link> was written. The addon consists of two pieces. The first is a client program (send_nsca) which is run from a remote
  host and is used to send the service check results to another server. The second piece is the nsca daemon (nsca) which either
  runs as a standalone daemon or under inetd and listens for connections from client programs. Upon receiving service check
  information from a client, the daemon will sumbit the check information to &name-icinga; (on the central server) by inserting a
  <emphasis>PROCESS_SVC_CHECK_RESULT</emphasis> command into the <link linkend="configmain-command_file">external command
  file</link>, along with the check results. The next time &name-icinga; checks for <link linkend="extcommands">external
  commands</link>, it will find the passive service check information that was sent from the distributed server and process it.
  Easy, huh?</para>

  <para><emphasis role="bold">Distributed Server Configuration</emphasis></para>

  <para>So how exactly is &name-icinga; configured on a distributed server? Basically, its just a bare-bones installation. You
  don't need to install the web interface or have notifications sent out from the server, as this will all be handled by the
  central server.</para>

  <para>Key configuration changes:</para>

  <itemizedlist>
    <listitem>
      <para>Only those services and hosts which are being monitored directly by the distributed server are defined in the <link
      linkend="configobject">object configuration file</link>.</para>
    </listitem>

    <listitem>
      <para>The distributed server has its <link linkend="configmain-enable_notifications">enable_notifications</link> directive
      set to 0. This will prevent any notifications from being sent out by the server.</para>
    </listitem>

    <listitem>
      <para>The distributed server is configured to <link linkend="configmain-obsess_over_services">obsess over
      services</link>.</para>
    </listitem>

    <listitem>
      <para>The distributed server has an <link linkend="configmain-ocsp_command">ocsp command</link> defined (as described
      below).</para>
    </listitem>
  </itemizedlist>

  <para>In order to make everything come together and work properly, we want the distributed server to report the results of
  <emphasis>all</emphasis> service checks to &name-icinga;. We could use <link linkend="eventhandlers">event handlers</link> to
  report <emphasis>changes</emphasis> in the state of a service, but that just doesn't cut it. In order to force the distributed
  server to report all service check results, you must enabled the <link
  linkend="configmain-obsess_over_services">obsess_over_services</link> option in the main configuration file and provide a <link
  linkend="configmain-ocsp_command">ocsp_command</link> to be run after every service check. We will use the ocsp command to send
  the results of all service checks to the central server, making use of the send_nsca client and nsca daemon (as described above)
  to handle the tranmission.</para>

  <para>In order to accomplish this, you'll need to define an ocsp command like this:</para>

  <para><emphasis role="bold">ocsp_command=submit_check_result</emphasis></para>

  <para>The command definition for the <emphasis>submit_check_result</emphasis> command looks something like this:</para>

  <screen> define command{
      command_name    submit_check_result 
      command_line    /usr/local/icinga/libexec/eventhandlers/submit_check_result $HOSTNAME$ '$SERVICEDESC$' $SERVICESTATE$ '$SERVICEOUTPUT$'
 } </screen>

  <para>The <emphasis>submit_check_result</emphasis> shell scripts looks something like this (replace
  <emphasis>central_server</emphasis> with the IP address of the central server):</para>

  <screen>#!/bin/sh

# Arguments:
#  $1 = host_name (Short name of host that the service is
#       associated with)
#  $2 = svc_description (Description of the service)
#  $3 = state_string (A string representing the status of
#       the given service - "OK", "WARNING", "CRITICAL"
#       or "UNKNOWN")
#  $4 = plugin_output (A text string that should be used
#       as the plugin output for the service checks)
#

# Convert the state string to the corresponding return code
return_code=-1

case "$3" in
    OK)
        return_code=0
        ;;
    WARNING)
        return_code=1
        ;;
    CRITICAL)
        return_code=2
        ;;
    UNKNOWN)
        return_code=-1
        ;;
esac

# pipe the service check info into the send_nsca program, which
# in turn transmits the data to the nsca daemon on the central
# monitoring server

/bin/printf "%s\t%s\t%s\t%s\n" "$1" "$2" "$return_code" "$4" | &url-icinga-base;/bin/send_nsca -H <emphasis> central_server</emphasis> -c &url-icinga-base;/etc/send_nsca.cfg</screen>

  <para>The script above assumes that you have the send_nsca program and it configuration file (send_nsca.cfg) located in the
  <emphasis>&url-icinga-base;/bin/</emphasis> and <emphasis>&url-icinga-base;/etc/</emphasis> directories, respectively.</para>

  <para>That's it! We've sucessfully configured a remote host running &name-icinga; to act as a distributed monitoring server.
  Let's go over exactly what happens with the distributed server and how it sends service check results to &name-icinga; (the
  steps outlined below correspond to the numbers in the reference diagram above):</para>

  <orderedlist>
    <listitem>
      <para>After the distributed server finishes executing a service check, it executes the command you defined by the <link
      linkend="configmain-ocsp_command">ocsp_command</link> variable. In our example, this is the
      <emphasis>&url-icinga-base;/libexec/eventhandlers/submit_check_result</emphasis> script. Note that the definition for the
      <emphasis>submit_check_result</emphasis> command passed four pieces of information to the script: the name of the host the
      service is associated with, the service description, the return code from the service check, and the plugin output from the
      service check.</para>
    </listitem>

    <listitem>
      <para>The <emphasis>submit_check_result</emphasis> script pipes the service check information (host name, description,
      return code, and output) to the <emphasis>send_nsca</emphasis> client program.</para>
    </listitem>

    <listitem>
      <para>The <emphasis>send_nsca</emphasis> program transmits the service check information to the <emphasis>nsca</emphasis>
      daemon on the central monitoring server.</para>
    </listitem>

    <listitem>
      <para>The <emphasis>nsca</emphasis> daemon on the central server takes the service check information and writes it to the
      external command file for later pickup by &name-icinga;.</para>
    </listitem>

    <listitem>
      <para>The &name-icinga; process on the central server reads the external command file and processes the passive service
      check information that originated from the distributed monitoring server.</para>
    </listitem>
  </orderedlist>

  <para><emphasis role="bold">Central Server Configuration</emphasis></para>

  <para>We've looked at how distributed monitoring servers should be configured, so let's turn to the central server. For all
  intensive purposes, the central is configured as you would normally configure a standalone server. It is setup as
  follows:</para>

  <itemizedlist>
    <listitem>
      <para>The central server has the web interface installed (optional, but recommended)</para>
    </listitem>

    <listitem>
      <para>The central server has its <link linkend="configmain-enable_notifications">enable_notifications</link> directive set
      to 1. This will enable notifications. (optional, but recommended)</para>
    </listitem>

    <listitem>
      <para>The central server has <link linkend="configmain-execute_service_checks">active service checks</link> disabled
      (optional, but recommended - see notes below)</para>
    </listitem>

    <listitem>
      <para>The central server has <link linkend="configmain-check_external_commands">external command checks</link> enabled
      (required)</para>
    </listitem>

    <listitem>
      <para>The central server has <link linkend="configmain-accept_passive_service_checks">passive service checks</link> enabled
      (required)</para>
    </listitem>
  </itemizedlist>

  <para>There are three other very important things that you need to keep in mind when configuring the central server:</para>

  <itemizedlist>
    <listitem>
      <para>The central server must have service definitions for <emphasis>all services</emphasis> that are being monitored by all
      the distributed servers. &name-icinga; will ignore passive check results if they do not correspond to a service that has
      been defined.</para>
    </listitem>

    <listitem>
      <para>If you're only using the central server to process services whose results are going to be provided by distributed
      hosts, you can simply disable all active service checks on a program-wide basis by setting the <link
      linkend="configmain-execute_service_checks">execute_service_checks</link> directive to 0. If you're using the central server
      to actively monitor a few services on its own (without the aid of distributed servers), the
      <emphasis>active_checks_enabled</emphasis> option of the definitions for service being monitored by distributed servers should
      be set to 0. This will prevent &name-icinga; from actively checking those services.</para>
    </listitem>
  </itemizedlist>

  <para>It is important that you either disable all service checks on a program-wide basis or disable the
  <emphasis>enable_active_checks</emphasis> option in the definitions for each service that is monitored by a distributed server.
  This will ensure that active service checks are never executed under normal circumstances. The services will keep getting
  rescheduled at their normal check intervals (3 minutes, 5 minutes, etc...), but the won't actually be executed. This
  rescheduling loop will just continue all the while &name-icinga; is running. We'll explain why this is done in a bit...</para>

  <para>That's it! Easy, huh?</para>

  <para><emphasis role="bold">Problems With Passive Checks</emphasis></para>

  <para>For all intensive purposes we can say that the central server is relying solely on passive checks for monitoring. The main
  problem with relying completely on passive checks for monitoring is the fact that &name-icinga; must rely on something else to
  provide the monitoring data. What if the remote host that is sending in passive check results goes down or becomes unreachable?
  If &name-icinga; isn't actively checking the services on the host, how will it know that there is a problem?</para>

  <para>Fortunately, there is a way we can handle these types of problems...</para>

  <para><emphasis role="bold">Freshness Checking</emphasis></para>

  <para>&name-icinga; supports a feature that does "freshness" checking on the results of service checks. More information
  freshness checking can be found <link linkend="freshness">here</link>. This features gives some protection against situations
  where remote hosts may stop sending passive service checks into the central monitoring server. The purpose of "freshness"
  checking is to ensure that service checks are either being provided passively by distributed servers on a regular basis or
  performed actively by the central server if the need arises. If the service check results provided by the distributed servers
  get "stale", &name-icinga; can be configured to force active checks of the service from the central monitoring host.</para>

  <para>So how do you do this? On the central monitoring server you need to configure services that are being monitoring by
  distributed servers as follows...</para>

  <itemizedlist>
    <listitem>
      <para>The <emphasis>check_freshness</emphasis> option in the service definitions should be set to 1. This enables
      "freshness" checking for the services.</para>
    </listitem>

    <listitem>
      <para>The <emphasis>freshness_threshold</emphasis> option in the service definitions should be set to a value (in seconds)
      which reflects how "fresh" the results for the services (provided by the distributed servers) should be.</para>
    </listitem>

    <listitem>
      <para>The <emphasis>check_command</emphasis> option in the service definitions should reflect valid commands that can be
      used to actively check the service from the central monitoring server.</para>
    </listitem>
  </itemizedlist>

  <para>&name-icinga; periodically checks the "freshness" of the results for all services that have freshness checking enabled.
  The <emphasis>freshness_threshold</emphasis> option in each service definition is used to determine how "fresh" the results for
  each service should be. For example, if you set this value to 300 for one of your services, &name-icinga; will consider the
  service results to be "stale" if they're older than 5 minutes (300 seconds). If you do not specify a value for the
  <emphasis>freshness_threshold</emphasis> option, &name-icinga; will automatically calculate a "freshness" threshold by looking
  at either the <emphasis>check_interval</emphasis> or <emphasis>retry_interval</emphasis> options (depending on what
  <link linkend="statetypes">type of state</link> the service is in). If the service results are found to be "stale",
  &name-icinga; will run the service check command specified by the <emphasis>check_command</emphasis> option in the service
  definition, thereby actively checking the service.</para>

  <para>Remember that you have to specify a <emphasis>check_command</emphasis> option in the service definitions that can be used
  to actively check the status of the service from the central monitoring server. Under normal circumstances, this check command
  is never executed (because active checks were disabled on a program-wide basis or for the specific services). When freshness
  checking is enabled, &name-icinga; will run this command to actively check the status of the service <emphasis>even if active
  checks are disabled on a program-wide or service-specific basis</emphasis>.</para>

  <para>If you are unable to define commands to actively check a service from the central monitoring host (or if turns out to be a
  major pain), you could simply define all your services with the <emphasis>check_command</emphasis> option set to run a dummy
  script that returns a critical status. Here's an example... Let's assume you define a command called 'service-is-stale' and use
  that command name in the <emphasis>check_command</emphasis> option of your services. Here's what the definition would look
  like...</para>

  <screen> define command{ 
     command_name service-is-stale 
     command_line /usr/local/icinga/libexec/check_dummy 2 "CRITICAL: Service results are stale"
 }</screen>

  <para>When &name-icinga; detects that the service results are stale and runs the <emphasis
  role="bold">service-is-stale</emphasis> command, the <emphasis>check_dummy</emphasis> plugin is executed and the service will go
  into a critical state. This would likely cause notifications to be sent out, so you'll know that there's a problem.</para>

  <para><emphasis role="bold">Performing Host Checks</emphasis></para>

  <para>At this point you know how to obtain service check results passivly from distributed servers. This means that the central
  server is not actively checking services on its own. But what about host checks? You still need to do them, so how?</para>

  <para>Since host checks usually compromise a small part of monitoring activity (they aren't done unless absolutely necessary),
  we'd recommend that you perform host checks actively from the central server. That means that you define host checks on the
  central server the same way that you do on the distributed servers (and the same way you would in a normal, non-distributed
  setup).</para>

  <para>Passive host checks are available (read <link linkend="passivechecks">here</link>), so you could use them in your
  distributed monitoring setup, but they suffer from a few problems. The biggest problem is that &name-icinga; does not translate
  passive host check problem states (DOWN and UNREACHABLE) when they are processed. This means that if your monitoring servers
  have a different parent/child host structure (and they will, if you monitoring servers are in different locations), the central
  monitoring server will have an inaccurate view of host states.</para>

  <para>If you do want to send passive host checks to a central server in your distributed monitoring setup, make sure:</para>

  <itemizedlist>
    <listitem>
      <para>The central server has <link linkend="configmain-accept_passive_host_checks">passive host checks</link> enabled
      (required)</para>
    </listitem>

    <listitem>
      <para>The distributed server is configured to <link linkend="configmain-obsess_over_hosts">obsess over hosts</link>.</para>
    </listitem>

    <listitem>
      <para>The distributed server has an <link linkend="configmain-ochp_command">ochp command</link> defined.</para>
    </listitem>
  </itemizedlist>

  <para>The ochp command, which is used for processing host check results, works in a similiar manner to the ocsp command, which
  is used for processing service check results (see documentation above). In order to make sure passive host check results are up
  to date, you'll want to enable <link linkend="freshness">freshness checking</link> for hosts (similiar to what is described
  above for services).</para>
  <indexterm zone="distributed_monitoring"><primary>Distributed Monitoring</primary></indexterm>
</section>
