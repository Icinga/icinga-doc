<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section [
<!ENTITY % all.entities SYSTEM "all-entities.ent">
%all.entities;
]>
<section version="5.0" xml:id="redundancy" xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:svg="http://www.w3.org/2000/svg"
         xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:html="http://www.w3.org/1999/xhtml"
         xmlns:db="http://docbook.org/ns/docbook">
  <title><anchor xml:id="redundancy_failover" />Redundant and Failover Network Monitoring</title>

  <para><emphasis role="bold">Introduction</emphasis></para>

  <para>This section describes a few scenarios for implementing redundant monitoring hosts an various types of network layouts.
  With redundant hosts, you can maintain the ability to monitor your network when the primary host that runs &name-icinga; fails
  or when portions of your network become unreachable.</para>

  <note>
    <para><emphasis condition="red" role="color"></emphasis> If you are just learning how to use &name-icinga;, we would suggest
    not trying to implement redundancy until you have becoming familiar with the <link
    linkend="redundancy-prerequisites">prerequisites</link> that have been laid out. Redundancy is a relatively complicated
    issue to understand, and even more difficult to implement properly.</para>
  </note>

  <para><emphasis role="bold">Index</emphasis></para>

  <para><link linkend="redundancy-prerequisites">Prerequisites</link></para>

  <para><link linkend="redundancy-samples">Sample scripts</link></para>

  <para><link linkend="redundancy-scenario_1">Scenario 1 - Redundant monitoring</link></para>

  <para><link linkend="redundancy-scenario_2">Scenario 2 - Failover monitoring</link></para>

  <para><anchor xml:id="redundancy-prerequisites" /></para>

  <informaltable frame="none">
    <tgroup cols="1">
      <colspec colname="c1" colwidth="100*" />

      <tbody>
        <row>
          <entry><para> <emphasis role="bold">Prerequisites</emphasis> </para></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>

  <para>Before you can even think about implementing redundancy with &name-icinga;, you need to be familiar with the
  following...</para>

  <itemizedlist>
    <listitem>
      <para>Implementing <link linkend="eventhandlers">event handlers</link> for hosts and services</para>
    </listitem>

    <listitem>
      <para>Issuing <link linkend="extcommands">external commands</link> to &name-icinga; via shell scripts</para>
    </listitem>

    <listitem>
      <para>Executing plugins on remote hosts using either the <link linkend="addons-nrpe">NRPE addon</link> or some other
      method</para>
    </listitem>

    <listitem>
      <para>Checking the status of the &name-icinga; process with the <emphasis>check_nagios</emphasis> plugin</para>
    </listitem>
  </itemizedlist>

  <para><anchor xml:id="redundancy-samples" /></para>

  <informaltable frame="none">
    <tgroup cols="1">
      <colspec colname="c1" colwidth="100*" />

      <tbody>
        <row>
          <entry><para> <emphasis role="bold">Sample Scripts</emphasis> </para></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>

  <para>All of the sample scripts that we use in this documentation can be found in the <emphasis>eventhandlers/</emphasis>
  subdirectory of the &name-icinga; distribution. You'll probably need to modify them to work on your system...</para>

  <para><anchor xml:id="redundancy-scenario_1" /></para>

  <informaltable frame="none">
    <tgroup cols="1">
      <colspec colname="c1" colwidth="100*" />

      <tbody>
        <row>
          <entry><para> <emphasis role="bold">Scenario 1 - Redundant Monitoring</emphasis> </para></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>

  <para><emphasis role="bold">Introduction</emphasis></para>

  <para>This is an easy (and naive) method of implementing redundant monitoring hosts on your network and it will only protect
  against a limited number of failures. More complex setups are necessary in order to provide smarter redundancy, better
  redundancy across different network segments, etc.</para>

  <para><emphasis role="bold">Goals</emphasis></para>

  <para>The goal of this type of redundancy implementation is simple. Both the "master" and "slave" hosts monitor the same hosts
  and service on the network. Under normal circumstances only the "master" host will be sending out notifications to contacts
  about problems. We want the "slave" host running &name-icinga; to take over the job of notifying contacts about problems
  if:</para>

  <orderedlist>
    <listitem>
      <para>The "master" host that runs &name-icinga; is down or..</para>
    </listitem>

    <listitem>
      <para>The &name-icinga; process on the "master" host stops running for some reason</para>
    </listitem>
  </orderedlist>

  <para><emphasis role="bold">Network Layout Diagram</emphasis></para>

  <para>The diagram below shows a very simple network setup. For this scenario we will be assuming that hosts A and E are both
  running &name-icinga; and are monitoring all the hosts shown. Host A will be considered the "master" host and host E will be
  considered the "slave" host.</para>

  <informaltable frame="all">
    <tgroup cols="1">
      <colspec colname="c1" colwidth="100*" />

      <tbody>
        <row>
          <entry><para> <inlinemediaobject>
              <imageobject>
                <imagedata fileref="../images/redundancy.png" format="PNG"></imagedata>
              </imageobject>
            </inlinemediaobject> </para></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>

  <para><emphasis role="bold">Initial Program Settings</emphasis></para>

  <para>The slave host (host E) has its initial <link linkend="configmain-enable_notifications">enable_notifications</link>
  directive disabled, thereby preventing it from sending out any host or service notifications. You also want to make sure that
  the slave host has its <link linkend="configmain-check_external_commands">check_external_commands</link> directive enabled. That
  was easy enough...</para>

  <para><emphasis role="bold">Initial Configuration</emphasis></para>

  <para>Next we need to consider the differences between the <link linkend="configobject">object configuration file(s)</link> on
  the master and slave hosts...</para>

  <para>We will assume that you have the master host (host A) setup to monitor services on all hosts shown in the diagram above.
  The slave host (host E) should be setup to monitor the same services and hosts, with the following additions in the
  configuration file...</para>

  <itemizedlist>
    <listitem>
      <para>The host definition for host A (in the host E configuration file) should have a host <link
      linkend="eventhandlers">event handler</link> defined. Lets say the name of the host event handler is <emphasis
      condition="red" role="color">handle-master-host-event</emphasis>.</para>
    </listitem>

    <listitem>
      <para>The configuration file on host E should have a service defined to check the status of the &name-icinga; process on
      host A. Lets assume that you define this service check to run the <emphasis>check_nagios</emphasis> plugin on host A. This
      can be done by using one of the methods described in <emphasis role="bold">this FAQ</emphasis> (update this!).</para>
    </listitem>

    <listitem>
      <para>The service definition for the &name-icinga; process check on host A should have an <link
      linkend="eventhandlers">event handler</link> defined. Lets say the name of the service event handler is <emphasis
      condition="red" role="color">handle-master-proc-event</emphasis>.</para>
    </listitem>
  </itemizedlist>

  <para>It is important to note that host A (the master host) has no knowledge of host E (the slave host). In this scenario it
  simply doesn't need to. Of course you may be monitoring services on host E from host A, but that has nothing to do with the
  implementation of redundancy...</para>

  <para><emphasis role="bold">Event Handler Command Definitions</emphasis></para>

  <para>We need to stop for a minute and describe what the command definitions for the event handlers on the slave host look like.
  Here is an example...</para>

  <screen> define command{
     command_name handle-master-host-event
     command_line /usr/local/icinga/libexec/eventhandlers/handle-master-host-event $HOSTSTATE$ $HOSTSTATETYPE$
 } 

 define command{
     command_name handle-master-proc-event 
     command_line /usr/local/icinga/libexec/eventhandlers/handle-master-proc-event $SERVICESTATE$ $SERVICESTATETYPE$
 }</screen>

  <para>This assumes that you have placed the event handler scripts in the
  <emphasis>&url-icinga-base;/libexec/eventhandlers</emphasis> directory. You may place them anywhere you wish, but you'll need to
  modify the examples that are given here.</para>

  <para><emphasis role="bold">Event Handler Scripts</emphasis></para>

  <para>Okay, now lets take a look at what the event handler scripts look like...</para>

  <para>Host Event Handler (<emphasis condition="red" role="color">handle-master-host-event</emphasis>):</para>

  <screen>#!/bin/sh

# Only take action on hard host states...
case "$2" in
HARD)
        case "$1" in
        DOWN)
                # The master host has gone down!
                # We should now become the master host and take
                # over the responsibilities of monitoring the 
                # network, so enable notifications...
                &url-icinga-base;/libexec/eventhandlers/enable_notifications
                ;;
        UP)
                # The master host has recovered!
                # We should go back to being the slave host and
                # let the master host do the monitoring, so 
                # disable notifications...
                &url-icinga-base;/libexec/eventhandlers/disable_notifications
                ;;
        esac
        ;;
esac
exit 0</screen>

  <para>Service Event Handler (<emphasis condition="red" role="color">handle-master-proc-event</emphasis>):</para>

  <screen>#!/bin/sh

# Only take action on hard service states...
case "$2" in
HARD)
        case "$1" in
        CRITICAL)
                # The master &name-icinga; process is not running!
                # We should now become the master host and
                # take over the responsibility of monitoring
                # the network, so enable notifications...
                &url-icinga-base;/libexec/eventhandlers/enable_notifications
                ;;
        WARNING)
        UNKNOWN)
                # The master &name-icinga; process may or may not
                # be running.. We won't do anything here, but
                # to be on the safe side you may decide you 
                # want the slave host to become the master in
                # these situations...
                ;;
        OK)
                # The master &name-icinga; process running again!
                # We should go back to being the slave host, 
                # so disable notifications...
                &url-icinga-base;/libexec/eventhandlers/disable_notifications
                ;;
        esac
        ;;
esac
exit 0</screen>

  <para><emphasis role="bold">What This Does For Us</emphasis></para>

  <para>The slave host (host E) initially has notifications disabled, so it won't send out any host or service notifications while
  the &name-icinga; process on the master host (host A) is still running.</para>

  <para>The &name-icinga; process on the slave host (host E) becomes the master host when...</para>

  <itemizedlist>
    <listitem>
      <para>The master host (host A) goes down and the <emphasis> <emphasis condition="red"
      role="color">handle-master-host-event</emphasis> </emphasis> host event handler is executed.</para>
    </listitem>

    <listitem>
      <para>The &name-icinga; process on the master host (host A) stops running and the <emphasis> <emphasis condition="red"
      role="color">handle-master-proc-event</emphasis> </emphasis> service event handler is executed.</para>
    </listitem>
  </itemizedlist>

  <para>When the &name-icinga; process on the slave host (host E) has notifications enabled, it will be able to send out
  notifications about any service or host problems or recoveries. At this point host E has effectively taken over the
  responsibility of notifying contacts of host and service problems!</para>

  <para>The &name-icinga; process on host E returns to being the slave host when...</para>

  <itemizedlist>
    <listitem>
      <para>Host A recovers and the <emphasis> <emphasis condition="red" role="color">handle-master-host-event</emphasis>
      </emphasis> host event handler is executed.</para>
    </listitem>

    <listitem>
      <para>The &name-icinga; process on host A recovers and the <emphasis> <emphasis condition="red"
      role="color">handle-master-proc-event</emphasis> </emphasis> service event handler is executed.</para>
    </listitem>
  </itemizedlist>

  <para>When the &name-icinga; process on host E has notifications disabled, it will not send out notifications about any service
  or host problems or recoveries. At this point host E has handed over the responsibilities of notifying contacts of problems to
  the &name-icinga; process on host A. Everything is now as it was when we first started!</para>

  <para><emphasis role="bold">Time Lags</emphasis></para>

  <para>Redundancy in &name-icinga; is by no means perfect. One of the more obvious problems is the lag time between the master
  host failing and the slave host taking over. This is affected by the following...</para>

  <itemizedlist>
    <listitem>
      <para>The time between a failure of the master host and the first time the slave host detects a problem</para>
    </listitem>

    <listitem>
      <para>The time needed to verify that the master host really does have a problem (using service or host check retries on the
      slave host)</para>
    </listitem>

    <listitem>
      <para>The time between the execution of the event handler and the next time that &name-icinga; checks for external
      commands</para>
    </listitem>
  </itemizedlist>

  <para>You can minimize this lag by...</para>

  <itemizedlist>
    <listitem>
      <para>Ensuring that the &name-icinga; process on host E (re)checks one or more services at a high frequency. This is done by
      using the <emphasis>check_interval</emphasis> and <emphasis>retry_interval</emphasis> arguments in each service
      definition.</para>
    </listitem>

    <listitem>
      <para>Ensuring that the number of host rechecks for host A (on host E) allow for fast detection of host problems. This is
      done by using the <emphasis>max_check_attempts</emphasis> argument in the host definition.</para>
    </listitem>

    <listitem>
      <para>Increase the frequency of <link linkend="extcommands">external command</link> checks on host E. This is done by
      modifying the <link linkend="configmain-command_check_interval">command_check_interval</link> option in the main
      configuration file.</para>
    </listitem>
  </itemizedlist>

  <para>When &name-icinga; recovers on the host A, there is also some lag time before host E returns to being a slave host. This
  is affected by the following...</para>

  <itemizedlist>
    <listitem>
      <para>The time between a recovery of host A and the time the &name-icinga; process on host E detects the recovery</para>
    </listitem>

    <listitem>
      <para>The time between the execution of the event handler on host B and the next time the &name-icinga; process on host E
      checks for external commands</para>
    </listitem>
  </itemizedlist>

  <para>The exact lag times between the transfer of monitoring responsibilities will vary depending on how many services you have
  defined, the interval at which services are checked, and a lot of pure chance. At any rate, its definitely better than
  nothing.</para>

  <para><emphasis role="bold">Special Cases</emphasis></para>

  <para>Here is one thing you should be aware of... If host A goes down, host E will have notifications enabled and take over the
  responsibilities of notifying contacts of problems. When host A recovers, host E will have notifications disabled. If - when
  host A recovers - the &name-icinga; process on host A does not start up properly, there will be a period of time when neither
  host is notifying contacts of problems! Fortunately, the service check logic in &name-icinga; accounts for this. The next time
  the &name-icinga; process on host E checks the status of the &name-icinga; process on host A, it will find that it is not
  running. Host E will then have notifications enabled again and take over all responsibilities of notifying contacts of
  problems.</para>

  <para>The exact amount of time that neither host is monitoring the network is hard to determine. Obviously, this period can be
  minimized by increasing the frequency of service checks (on host E) of the &name-icinga; process on host A. The rest is up to
  pure chance, but the total "blackout" time shouldn't be too bad.</para>

  <para><anchor xml:id="redundancy-scenario_2" /></para>

  <informaltable frame="none">
    <tgroup cols="1">
      <colspec colname="c1" colwidth="100*" />

      <tbody>
        <row>
          <entry><para> <emphasis role="bold">Scenario 2 - Failover Monitoring</emphasis> </para></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>

  <para><emphasis role="bold">Introduction</emphasis></para>

  <para>Failover monitoring is similiar to, but slightly different than redundant monitoring (as discussed above in <link
  linkend="redundancy-scenario_1">scenario 1</link>).</para>

  <para><emphasis role="bold">Goals</emphasis></para>

  <para>The basic goal of failover monitoring is to have the &name-icinga; process on the slave host sit idle while the
  &name-icinga; process on the master host is running. If the process on the master host stops running (or if the host goes down),
  the &name-icinga; process on the slave host starts monitoring everything.</para>

  <para>While the method described in <link linkend="redundancy-scenario_1">scenario 1</link> will allow you to continue
  receive notifications if the master monitoring hosts goes down, it does have some pitfalls. The biggest problem is that the
  slave host is monitoring the same hosts and servers as the master <emphasis>at the same time as the master</emphasis>! This can
  cause problems with excessive traffic and load on the machines being monitored if you have a lot of services defined. Here's how
  you can get around that problem...</para>

  <para><emphasis role="bold">Initial Program Settings</emphasis></para>

  <para>Disable active service checks and notifications on the slave host using the <link
  linkend="configmain-execute_service_checks">execute_service_checks</link> and <link
  linkend="configmain-enable_notifications">enable_notifications</link> directives. This will prevent the slave host from
  monitoring hosts and services and sending out notifications while the &name-icinga; process on the master host is still up and
  running. Make sure you also have the <link linkend="configmain-check_external_commands">check_external_commands</link> directive
  enabled on the slave host.</para>

  <para><emphasis role="bold">Master Process Check</emphasis></para>

  <para>Set up a cron job on the slave host that periodically (say every minute) runs a script that checks the status of the
  &name-icinga; process on the master host (using the <emphasis>check_nrpe</emphasis> plugin on the slave host and the <link
  linkend="addons-nrpe">nrpe daemon</link> and <emphasis>check_nagios</emphasis> plugin on the master host). The script should
  check the return code of the <emphasis>check_nrpe plugin</emphasis> . If it returns a non-OK state, the script should send the
  appropriate commands to the <link linkend="configmain-command_file">external command file</link> to enable both notifications
  and active service checks. If the plugin returns an OK state, the script should send commands to the external command file to
  disable both notifications and active checks.</para>

  <para>By doing this you end up with only one process monitoring hosts and services at a time, which is much more efficient that
  monitoring everything twice.</para>

  <para>Also of note, you <emphasis>don't</emphasis> need to define host and service handlers as mentioned in <link
  linkend="redundancy-scenario_1">scenario 1</link> because things are handled differently.</para>

  <para><emphasis role="bold">Additional Issues</emphasis></para>

  <para>At this point, you have implemented a very basic failover monitoring setup. However, there is one more thing you should
  consider doing to make things work smoother.</para>

  <para>The big problem with the way things have been setup thus far is the fact that the slave host doesn't have the current
  status of any services or hosts at the time it takes over the job of monitoring. One way to solve this problem is to enable the
  <link linkend="configmain-ocsp_command">ocsp command</link> on the master host and have it send all service check results to the
  slave host using the <link linkend="addons-nsca">nsca addon</link>. The slave host will then have up-to-date status information
  for all services at the time it takes over the job of monitoring things. Since active service checks are not enabled on the
  slave host, it will not actively run any service checks. However, it will execute host checks if necessary. This means that both
  the master and slave hosts will be executing host checks as needed, which is not really a big deal since the majority of
  monitoring deals with service checks.</para>

  <para>That's pretty much it as far as setup goes.</para>
  <indexterm zone="redundancy_failover"><primary>Failover</primary><secondary>Redundant and Failover Network Monitoring</secondary></indexterm>
  <indexterm zone="redundancy_failover"><primary>Monitoring</primary><secondary>Redundant and Failover Network Monitoring</secondary></indexterm>
  <indexterm zone="redundancy_failover"><primary>Redundancy</primary><secondary>Redundant and Failover Network Monitoring</secondary></indexterm>
</section>
